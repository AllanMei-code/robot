# 前端来源（支持单值或多值）。
# 兼容老变量 FRONTEND_ORIGIN；推荐使用 FRONTEND_ORIGINS 以逗号分隔多个来源。
FRONTEND_ORIGINS=http://localhost:3000
FRONTEND_ORIGIN=
API_BASE_URL=http://127.0.0.1:5000

# OpenAI 兼容模型服务（llama.cpp server）
LLM_BASE_URL=http://127.0.0.1:8080/v1
LLM_API_KEY=sk-noauth
LLM_MODEL=qwen2.5-3b-instruct-q5_k_m
LLM_FALLBACK_ENABLED=true
LLM_MAX_RETRIES=0
# For llama.cpp OpenAI server chat template – set to qwen for Qwen/Qwen2 models
# Valid examples: qwen, mistral-instruct, chatml, llama-3, vicuna, ...
LLM_CHAT_FORMAT=qwen

# 翻译与会话参数
TRANSLATION_ENABLED=true
BOT_INACTIVITY_SEC=30
BOT_SUPPRESS_SEC=5

# 可自定义翻译端点（逗号分隔）
# 若你自建 LibreTranslate（例如在本机 5005）：
#   LIBRE_ENDPOINTS=http://127.0.0.1:5005/translate
# 并且为语言检测显式指定 /detect：
#   LIBRE_DETECT_ENDPOINTS=http://127.0.0.1:5005/detect
# 如使用本地 LibreTranslate，建议只保留本地端点：
# LIBRE_ENDPOINTS=http://127.0.0.1:5005/translate
# LIBRE_DETECT_ENDPOINTS=http://127.0.0.1:5005/detect
LIBRE_ENDPOINTS=https://libretranslate.de/translate,https://translate.astian.org/translate,https://libretranslate.com/translate
# LIBRE_DETECT_ENDPOINTS= 
